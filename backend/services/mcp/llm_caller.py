"""
LLM è°ƒç”¨åŒ…è£…å™¨

æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ï¼Œæ”¯æŒæ™®é€šèŠå¤©å’Œ Tool Callingã€‚
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional

from services.providers.factory import create_provider
from services.providers.base import LLMMessage, LLMResponse


@dataclass
class LLMCallResult:
    """LLM è°ƒç”¨ç»“æœ"""
    success: bool
    content: str = ""
    tool_calls: Optional[List[Dict[str, Any]]] = None
    finish_reason: Optional[str] = None
    error: Optional[str] = None


class LLMCaller:
    """
    LLM è°ƒç”¨å™¨
    
    å°è£… Provider SDK è°ƒç”¨ï¼Œæä¾›ç®€æ´çš„æ¥å£ã€‚
    
    Example:
        caller = LLMCaller(llm_config)
        result = caller.chat(system_prompt, user_input)
        result = caller.chat_with_tools(messages, tools)
    """
    
    def __init__(self, config: Dict[str, Any], log_func: Optional[Callable] = None):
        """
        Args:
            config: LLM é…ç½®ï¼ˆåŒ…å« provider, api_key, model ç­‰ï¼‰
            log_func: æ—¥å¿—å‡½æ•°ï¼ˆå¯é€‰ï¼‰
        """
        self._config = config
        self._log = log_func or (lambda x: None)
        
        self._provider = config.get('provider', '')
        self._api_key = config.get('api_key', '')
        self._api_url = config.get('api_url')
        self._model = config.get('model', '')
    
    def _validate_config(self) -> Optional[str]:
        """éªŒè¯é…ç½®ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯æˆ– None"""
        if not self._provider:
            return "ç¼ºå°‘ provider"
        if not self._api_key:
            return "ç¼ºå°‘ api_key"
        if not self._model:
            return "ç¼ºå°‘ model"
        return None
    
    def _create_provider(self):
        """åˆ›å»º Provider å®ä¾‹"""
        return create_provider(
            provider_type=self._provider,
            api_key=self._api_key,
            api_url=self._api_url,
            model=self._model,
        )
    
    def chat(
        self,
        system_prompt: str,
        user_input: str,
        temperature: float = 0.1,
        max_tokens: int = 8192,
    ) -> LLMCallResult:
        """
        æ™®é€šèŠå¤©è°ƒç”¨
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_input: ç”¨æˆ·è¾“å…¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            
        Returns:
            LLMCallResult
        """
        error = self._validate_config()
        if error:
            self._log(f"âŒ {error}")
            return LLMCallResult(success=False, error=error)
        
        try:
            provider = self._create_provider()
            
            messages = [
                LLMMessage(role='system', content=system_prompt),
                LLMMessage(role='user', content=user_input),
            ]
            
            self._log(f"ğŸ”„ è°ƒç”¨ {self._provider}/{self._model}")
            response = provider.chat(
                messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            self._log(f"âœ… è¿”å› {len(response.content or '')} å­—ç¬¦")
            
            return LLMCallResult(
                success=True,
                content=response.content or "",
                finish_reason=response.finish_reason,
            )
            
        except Exception as e:
            error_msg = f"{type(e).__name__}: {e}"
            self._log(f"âŒ {error_msg}")
            return LLMCallResult(success=False, error=error_msg)
    
    def chat_with_tools(
        self,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]],
        tool_choice: str = "auto",
        temperature: float = 0.1,
        max_tokens: int = 4096,
    ) -> LLMCallResult:
        """
        Tool Calling è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼‰
            tools: å·¥å…·åˆ—è¡¨ï¼ˆOpenAI function calling æ ¼å¼ï¼‰
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            
        Returns:
            LLMCallResult
        """
        error = self._validate_config()
        if error:
            self._log(f"âŒ {error}")
            return LLMCallResult(success=False, error=error)
        
        try:
            provider = self._create_provider()
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            llm_messages = [
                LLMMessage(
                    role=msg.get('role', 'user'),
                    content=msg.get('content', ''),
                    tool_calls=msg.get('tool_calls'),
                    tool_call_id=msg.get('tool_call_id'),
                    name=msg.get('name'),
                )
                for msg in messages
            ]
            
            self._log(f"ğŸ”§ Tool Calling: {len(tools)} å·¥å…·")
            response = provider.chat(
                llm_messages,
                tools=tools,
                tool_choice=tool_choice,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            tool_count = len(response.tool_calls or [])
            self._log(f"âœ… {tool_count} ä¸ªå·¥å…·è°ƒç”¨")
            
            return LLMCallResult(
                success=True,
                content=response.content or "",
                tool_calls=response.tool_calls,
                finish_reason=response.finish_reason,
            )
            
        except Exception as e:
            error_msg = f"{type(e).__name__}: {e}"
            self._log(f"âŒ {error_msg}")
            return LLMCallResult(success=False, error=error_msg)


# ==================== ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰ ====================

def call_llm_api(
    llm_config: Dict[str, Any],
    system_prompt: str,
    user_input: str,
    add_log: Optional[Callable] = None,
) -> Optional[str]:
    """
    è°ƒç”¨ LLM APIï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
    
    Args:
        llm_config: LLM é…ç½®
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_input: ç”¨æˆ·è¾“å…¥
        add_log: æ—¥å¿—å‡½æ•°
        
    Returns:
        å“åº”å†…å®¹æˆ– None
    """
    caller = LLMCaller(llm_config, add_log)
    result = caller.chat(system_prompt, user_input)
    return result.content if result.success else None


def call_llm_with_tools(
    llm_config: Dict[str, Any],
    messages: List[Dict[str, Any]],
    tools: List[Dict[str, Any]],
    add_log: Optional[Callable] = None,
) -> Optional[Dict[str, Any]]:
    """
    ä½¿ç”¨åŸç”Ÿ Tool Calling è°ƒç”¨ LLMï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
    
    Args:
        llm_config: LLM é…ç½®
        messages: æ¶ˆæ¯åˆ—è¡¨
        tools: å·¥å…·åˆ—è¡¨
        add_log: æ—¥å¿—å‡½æ•°
        
    Returns:
        {'content', 'tool_calls', 'finish_reason'} æˆ– None
    """
    caller = LLMCaller(llm_config, add_log)
    result = caller.chat_with_tools(messages, tools)
    
    if not result.success:
        return None
    
    return {
        'content': result.content,
        'tool_calls': result.tool_calls or [],
        'finish_reason': result.finish_reason,
    }
