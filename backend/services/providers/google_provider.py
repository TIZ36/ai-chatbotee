"""
Google Gemini Provider

æ”¯æŒ Gemini æ¨¡å‹ï¼ŒåŒ…æ‹¬å›¾ç‰‡ç”Ÿæˆ
ä¼˜å…ˆä½¿ç”¨ google-genai SDKï¼Œå›é€€åˆ° REST API
"""

from typing import List, Optional, Dict, Any, Generator, Tuple
import json
import base64
import time
import requests

from .base import BaseLLMProvider, LLMMessage, LLMResponse

# æ¨¡å—çº§ç¼“å­˜: (api_url, api_key) -> (timestamp, list_models_result)ï¼ŒTTL 5 åˆ†é’Ÿ
_LIST_MODELS_CACHE: Dict[Tuple[str, str], Tuple[float, List[Dict[str, Any]]]] = {}
_LIST_MODELS_CACHE_TTL = 300


class GoogleProvider(BaseLLMProvider):
    """Google Gemini Provider"""
    
    provider_type = "gemini"
    sdk_available = False
    
    def __init__(self, api_key: str, api_url: Optional[str] = None,
                 model: Optional[str] = None, **kwargs):
        self._client = None
        self._types = None
        self.use_thoughtsig = kwargs.get('use_thoughtsig', True)  # ç­¾åå¼€å…³ï¼Œé»˜è®¤å¼€å¯
        # è”ç½‘æœç´¢ (Google Search Grounding)ï¼šä» metadata.enableGoogleSearch æˆ– enable_google_search è¯»å–
        self._enable_google_search = kwargs.get('enableGoogleSearch', False) or kwargs.get('enable_google_search', False)
        super().__init__(api_key, api_url, model, **kwargs)
    
    def _init_sdk(self):
        """åˆå§‹åŒ– Google GenAI SDK"""
        try:
            from google import genai
            from google.genai import types
            import os
            
            # æ„å»º http_options
            http_options = {'api_version': 'v1beta'}
            
            # 1. æ£€æŸ¥æ˜¯å¦é…ç½®äº†è‡ªå®šä¹‰ API URL
            if self.api_url:
                base_url = self.api_url.rstrip('/')
                # å¦‚æœ URL åŒ…å« /v1beta æˆ– /v1ï¼Œå»æ‰å®ƒï¼ˆSDK ä¼šè‡ªå·±åŠ ï¼‰
                if '/v1beta' in base_url:
                    base_url = base_url.split('/v1beta')[0]
                elif '/v1' in base_url:
                    base_url = base_url.split('/v1')[0]
                
                # æ£€æµ‹æ˜¯å¦æ˜¯å®˜æ–¹åœ°å€
                is_official = 'generativelanguage.googleapis.com' in base_url
                
                http_options['base_url'] = base_url
                if is_official:
                    self._log(f"âœ… Using official API URL: {base_url}")
                    print(f"[GEMINIProvider] âœ… SDK ä½¿ç”¨å®˜æ–¹ API: {base_url}")
                else:
                    self._log(f"âœ… Using custom API URL (proxy): {base_url}")
                    print(f"[GEMINIProvider] âœ… SDK ä½¿ç”¨ä»£ç†: {base_url}")
            
            # 2. æ£€æŸ¥ç³»ç»Ÿä»£ç†ç¯å¢ƒå˜é‡ï¼ˆHTTP_PROXY / HTTPS_PROXYï¼‰
            http_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY') or \
                         os.environ.get('https_proxy') or os.environ.get('http_proxy')
            
            if http_proxy:
                # é…ç½® httpx å®¢æˆ·ç«¯ä½¿ç”¨ä»£ç†
                http_options['client_args'] = {
                    'proxy': http_proxy,
                    'timeout': 120.0,
                }
                self._log(f"âœ… Using system proxy: {http_proxy}")
                print(f"[GEMINIProvider] âœ… SDK ä½¿ç”¨ç³»ç»Ÿä»£ç†: {http_proxy}")
            
            # 3. åˆ›å»º Client
            if http_options.get('base_url') or http_options.get('client_args'):
                self._client = genai.Client(
                    api_key=self.api_key,
                    http_options=http_options
                )
                self._log("SDK initialized with custom http_options")
            else:
                # æ— ä»£ç†é…ç½®ï¼Œç›´æ¥ä½¿ç”¨å®˜æ–¹ API
                self._log("âš ï¸ No proxy configured, using official API directly")
                print("[GEMINIProvider] âš ï¸ æœªæ£€æµ‹åˆ°ä»£ç†é…ç½®ï¼Œç›´æ¥ä½¿ç”¨å®˜æ–¹ API")
                print("[GEMINIProvider] ğŸ’¡ æç¤ºï¼šå¦‚é‡åœ°åŒºé™åˆ¶ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ HTTPS_PROXY æˆ–åœ¨ LLM é…ç½®ä¸­è®¾ç½® api_url")
                self._client = genai.Client(api_key=self.api_key)
            
            self._types = types
            self.sdk_available = True
            self._log(f"SDK initialized successfully")
        except ImportError:
            self._log("google-genai SDK not installed, using REST API")
            self.sdk_available = False
        except Exception as e:
            self._log_error(f"Failed to initialize SDK: {e}")
            self.sdk_available = False
    
    def chat(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """éæµå¼èŠå¤©"""
        if self.sdk_available and self._client:
            return self._chat_sdk(messages, **kwargs)
        return self._chat_rest(messages, **kwargs)
    
    def chat_stream(self, messages: List[LLMMessage], **kwargs) -> Generator[str, None, LLMResponse]:
        """æµå¼èŠå¤©"""
        # å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨éæµå¼
        if self._is_image_generation_model():
            self._log("Image generation model detected, using non-streaming")
            response = self.chat(messages, **kwargs)
            if response.content:
                yield response.content
            return response
        
        if self.sdk_available and self._client:
            yield from self._chat_stream_sdk(messages, **kwargs)
        else:
            yield from self._chat_stream_rest(messages, **kwargs)
    
    def _is_image_generation_model(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡ç”Ÿæˆæ¨¡å‹"""
        return self.model and 'image' in self.model.lower()

    def _requires_thought_signature_workaround(self) -> bool:
        """
        éƒ¨åˆ† Gemini å›¾åƒ/å¤šæ¨¡æ€æ¨¡å‹åœ¨â€œå›çŒå†å² model è¾“å‡ºâ€æ—¶ï¼Œä¼šè¦æ±‚æ¯ä¸ª part éƒ½æºå¸¦ thoughtSignatureï¼Œ
        ä½†å†å²æ•°æ®é€šå¸¸æ²¡æœ‰ä¿å­˜ text part çš„ thoughtSignatureï¼Œå¯¼è‡´ 400 INVALID_ARGUMENTã€‚

        è¿™é‡Œåšä¸€ä¸ªä¿å®ˆçš„å…¼å®¹ç­–ç•¥ï¼šå½“ä½¿ç”¨å›¾åƒç›¸å…³æ¨¡å‹æ—¶ï¼ŒæŠŠå†å² assistant/tool çš„æ–‡æœ¬é™çº§ä¸º user role å‘é€ï¼Œ
        ä»¥é¿å…è§¦å‘ thoughtSignature å¼ºæ ¡éªŒï¼ŒåŒæ—¶ä¸å½±å“ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ï¼ˆå›¾ç”Ÿå›¾ï¼‰ã€‚
        """
        m = (self.model or '').lower()
        return ('image' in m) or ('image-preview' in m)
    
    def _chat_sdk(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """ä½¿ç”¨ SDK çš„éæµå¼èŠå¤©"""
        try:
            contents, system_instruction = self._convert_messages_for_gemini_sdk(messages)
            
            # æ„å»ºé…ç½®
            config = {}
            if system_instruction:
                config['system_instruction'] = system_instruction
            
            # å›¾ç‰‡ç”Ÿæˆæ¨¡å‹é…ç½®
            if self._is_image_generation_model():
                config['response_modalities'] = ['TEXT', 'IMAGE']
                self._log("Enabled response_modalities: ['TEXT', 'IMAGE']")
            elif self._enable_google_search and self._types:
                # è”ç½‘æœç´¢ (Google Search Grounding)ï¼Œä»…éå›¾ç‰‡æ¨¡å‹
                try:
                    config['tools'] = [self._types.Tool(google_search=self._types.GoogleSearch())]
                    self._log("Enabled Google Search Grounding")
                except Exception as e:
                    self._log(f"Could not add google_search tool: {e}")
            
            self._log(f"Calling SDK: model={self.model}, contents={len(contents)}")
            
            # è°ƒç”¨ API
            response = self._client.models.generate_content(
                model=self.model or 'gemini-2.5-flash',
                contents=contents,
                config=self._types.GenerateContentConfig(**config) if config else None
            )
            
            # è§£æå“åº”
            content = ""
            media = []
            
            if response.candidates:
                for candidate in response.candidates:
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                content += part.text
                            elif hasattr(part, 'inline_data') and part.inline_data:
                                mime_type = part.inline_data.mime_type or 'image/png'
                                image_data = part.inline_data.data
                                if isinstance(image_data, bytes):
                                    data = base64.b64encode(image_data).decode('utf-8')
                                else:
                                    data = image_data

                                # æå– thoughtSignatureï¼ˆGemini 2.5+ï¼‰
                                thought_sig = None
                                if hasattr(part, 'thought_signature') and part.thought_signature:
                                    thought_sig = part.thought_signature
                                    self._log(f"Found thoughtSignature in image: {len(thought_sig)} chars")

                                # ä¿å­˜åˆ° media åˆ—è¡¨ï¼ˆå‰ç«¯ç”¨ç¼©ç•¥å›¾/é¢„è§ˆå±•ç¤ºï¼Œä¸å†å¾€ Markdown content é‡Œå¡å›¾ï¼‰
                                media_item = {
                                    'type': 'image',
                                    'mimeType': mime_type,
                                    'data': data
                                }
                                # ä¿å­˜ thoughtSignatureï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œä¾›åç»­è¯·æ±‚ä½¿ç”¨
                                if thought_sig:
                                    media_item['thoughtSignature'] = thought_sig
                                    self._log(f"âœ… å›¾ç‰‡åŒ…å« thoughtSignature ({len(thought_sig)} å­—ç¬¦)")
                                else:
                                    self._log(f"âš ï¸ å›¾ç‰‡ä¸åŒ…å« thoughtSignature")
                                media.append(media_item)
                                self._log(f"Received image: {mime_type} ({len(data)} chars)")
            
            return LLMResponse(
                content=content,
                media=media if media else None,
                finish_reason=response.candidates[0].finish_reason if response.candidates else None
            )
        except Exception as e:
            self._log_error(f"SDK chat error: {e}", e)
            detail = str(e) or repr(e)
            raise RuntimeError(f"Google API error: {detail}")
    
    def _chat_stream_sdk(self, messages: List[LLMMessage], **kwargs) -> Generator[str, None, LLMResponse]:
        """ä½¿ç”¨ SDK çš„æµå¼èŠå¤©"""
        try:
            contents, system_instruction = self._convert_messages_for_gemini_sdk(messages)
            
            # æ„å»ºé…ç½®
            config = {}
            if system_instruction:
                config['system_instruction'] = system_instruction
            
            # å›¾ç‰‡ç”Ÿæˆæ¨¡å‹é…ç½®
            if self._is_image_generation_model():
                config['response_modalities'] = ['TEXT', 'IMAGE']
            elif self._enable_google_search and self._types:
                try:
                    config['tools'] = [self._types.Tool(google_search=self._types.GoogleSearch())]
                    self._log("Enabled Google Search Grounding (stream)")
                except Exception as e:
                    self._log(f"Could not add google_search tool: {e}")
            
            self._log(f"Calling SDK stream: model={self.model}")
            
            # æµå¼è°ƒç”¨
            stream = self._client.models.generate_content_stream(
                model=self.model or 'gemini-2.5-flash',
                contents=contents,
                config=self._types.GenerateContentConfig(**config) if config else None
            )
            
            full_content = ""
            finish_reason = None
            media = []
            
            for chunk in stream:
                if chunk.candidates:
                    for candidate in chunk.candidates:
                        if candidate.content and candidate.content.parts:
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    full_content += part.text
                                    yield part.text
                                elif hasattr(part, 'inline_data') and part.inline_data:
                                    # æå–å›¾ç‰‡æ•°æ®
                                    mime_type = part.inline_data.mime_type or 'image/png'
                                    image_data = part.inline_data.data
                                    if isinstance(image_data, bytes):
                                        data = base64.b64encode(image_data).decode('utf-8')
                                    else:
                                        data = image_data
                                    
                                    # æå– thoughtSignatureï¼ˆGemini 2.5+ï¼‰
                                    thought_sig = None
                                    if hasattr(part, 'thought_signature') and part.thought_signature:
                                        thought_sig = part.thought_signature
                                        self._log(f"[Stream] Found thoughtSignature in image: {len(thought_sig)} chars")
                                    
                                    media_item = {
                                        'type': 'image',
                                        'mimeType': mime_type,
                                        'data': data
                                    }
                                    if thought_sig:
                                        media_item['thoughtSignature'] = thought_sig
                                        self._log(f"âœ… å›¾ç‰‡åŒ…å« thoughtSignature ({len(thought_sig)} å­—ç¬¦)")
                                    else:
                                        self._log(f"âš ï¸ å›¾ç‰‡ä¸åŒ…å« thoughtSignature")
                                    media.append(media_item)
                                    self._log(f"[Stream] Received image: {mime_type} ({len(data)} chars)")
                        if candidate.finish_reason:
                            finish_reason = candidate.finish_reason
            
            return LLMResponse(
                content=full_content,
                media=media if media else None,
                finish_reason=finish_reason
            )
        except Exception as e:
            self._log_error(f"SDK stream error: {e}", e)
            detail = str(e) or repr(e)
            raise RuntimeError(f"Google API error: {detail}")
    
    def _chat_rest(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """ä½¿ç”¨ REST API çš„éæµå¼èŠå¤©"""
        url = self._get_api_url(stream=False)
        contents, system_instruction = self._convert_messages_for_gemini_rest(messages)
        
        payload = {'contents': contents}
        if system_instruction:
            # Gemini REST API ä½¿ç”¨ systemInstructionï¼ˆcamelCaseï¼‰
            payload['systemInstruction'] = system_instruction
        
        # å›¾ç‰‡ç”Ÿæˆæ¨¡å‹é…ç½®
        if self._is_image_generation_model():
            payload['generationConfig'] = {
                'responseModalities': ['TEXT', 'IMAGE']
            }
        
        self._log(f"Calling REST API: {url.split('?')[0]}...")
        
        response = requests.post(url, json=payload, timeout=120)
        
        if response.status_code != 200:
            error_text = self._parse_error_response(response)
            raise RuntimeError(f"Google API error: {error_text}")
        
        data = response.json()
        content = ""
        media = []
        
        candidates = data.get('candidates', [])
        for candidate in candidates:
            parts = candidate.get('content', {}).get('parts', [])
            for part in parts:
                if 'text' in part:
                    content += part['text']
                elif 'inlineData' in part:
                    media_item = {
                        'type': 'image',
                        'mimeType': part['inlineData'].get('mimeType', 'image/png'),
                        'data': part['inlineData'].get('data', '')
                    }
                    # æå– thoughtSignatureï¼ˆGemini 2.5+ï¼‰- REST API æ ¼å¼
                    thought_sig = part.get('thoughtSignature')
                    if thought_sig:
                        media_item['thoughtSignature'] = thought_sig
                        self._log(f"[REST] Found thoughtSignature in image: {len(thought_sig)} chars")
                        self._log(f"âœ… å›¾ç‰‡åŒ…å« thoughtSignature ({len(thought_sig)} å­—ç¬¦)")
                    else:
                        self._log(f"âš ï¸ å›¾ç‰‡ä¸åŒ…å« thoughtSignature")
                    media.append(media_item)
        
        return LLMResponse(
            content=content,
            media=media if media else None,
            raw=data
        )
    
    def _chat_stream_rest(self, messages: List[LLMMessage], **kwargs) -> Generator[str, None, LLMResponse]:
        """ä½¿ç”¨ REST API çš„æµå¼èŠå¤©"""
        url = self._get_api_url(stream=True)
        contents, system_instruction = self._convert_messages_for_gemini_rest(messages)
        
        payload = {'contents': contents}
        if system_instruction:
            # Gemini REST API ä½¿ç”¨ systemInstructionï¼ˆcamelCaseï¼‰
            payload['systemInstruction'] = system_instruction
        
        response = requests.post(url, json=payload, stream=True, timeout=120)
        
        if response.status_code != 200:
            error_text = self._parse_error_response(response)
            raise RuntimeError(f"Google API error: {error_text}")
        
        full_content = ""
        finish_reason = None
        
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    try:
                        chunk = json.loads(line[6:])
                        candidates = chunk.get('candidates', [])
                        if candidates:
                            parts = candidates[0].get('content', {}).get('parts', [])
                            for part in parts:
                                if 'text' in part:
                                    text = part['text']
                                    full_content += text
                                    yield text
                            if candidates[0].get('finishReason'):
                                finish_reason = candidates[0]['finishReason']
                    except json.JSONDecodeError:
                        continue
        
        return LLMResponse(
            content=full_content,
            finish_reason=finish_reason
        )
    
    def _get_api_url(self, stream: bool = False) -> str:
        """è·å– API URL"""
        model = self.model or 'gemini-2.5-flash'
        endpoint = 'streamGenerateContent?alt=sse' if stream else 'generateContent'
        
        if self.api_url:
            base = self.api_url.rstrip('/')
            url = f"{base}/models/{model}:{endpoint}"
        else:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:{endpoint}"
        
        # æ·»åŠ  API key
        separator = '&' if '?' in url else '?'
        url += f"{separator}key={self.api_key}"
        
        return url
    
    def _find_recent_thought_signature(self, messages: List[LLMMessage]) -> Optional[str]:
        """
        æŸ¥æ‰¾æœ€è¿‘çš„ LLM è¾“å‡ºçš„å¸¦æœ‰ thought signature çš„å›¾ç‰‡çš„ç­¾å
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æœ€è¿‘çš„ thought signatureï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å› None
        """
        # ä»åå¾€å‰æŸ¥æ‰¾ï¼ˆæœ€è¿‘çš„ä¼˜å…ˆï¼‰
        for msg in reversed(messages):
            if msg.role in ('assistant', 'model') and msg.media:
                for media_item in msg.media:
                    if isinstance(media_item, dict):
                        thought_sig = media_item.get('thoughtSignature') or media_item.get('thought_signature')
                        if thought_sig:
                            return thought_sig
        return None
    
    def _convert_messages_for_gemini_sdk(self, messages: List[LLMMessage]) -> tuple:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Gemini SDK æ ¼å¼"""
        contents = []
        system_instruction = None
        
        # æŸ¥æ‰¾æœ€è¿‘çš„ thought signatureï¼ˆç”¨äºç­¾åå¼€å…³æ‰“å¼€æ—¶çš„å‚è€ƒï¼‰
        recent_thought_sig = self._find_recent_thought_signature(messages)
        if recent_thought_sig:
            self._log(f"æ‰¾åˆ°æœ€è¿‘çš„ thoughtSignature å‚è€ƒ: {len(recent_thought_sig)} å­—ç¬¦")
        
        # é¢„æ£€æŸ¥ï¼šæ‰“å°å“ªäº›æ¶ˆæ¯åŒ…å« thoughtSignature
        self._log("=== ThoughtSignature æ£€æŸ¥ ===")
        content_idx = 0
        for i, msg in enumerate(messages):
            if msg.role == 'system':
                continue
            has_sig = False
            sig_count = 0
            if msg.media:
                for media_item in msg.media:
                    if isinstance(media_item, dict):
                        if media_item.get('thoughtSignature') or media_item.get('thought_signature'):
                            has_sig = True
                            sig_count += 1
            media_count = len(msg.media) if msg.media else 0
            status = f"âœ… {sig_count} sig" if has_sig else (f"âš ï¸ æ— sig ({media_count}å›¾)" if media_count > 0 else "")
            if media_count > 0:
                self._log(f"  Content[{content_idx}] {msg.role}: {media_count}å¼ å›¾ {status}")
            content_idx += 1
        self._log("=============================")
        
        for msg in messages:
            if msg.role == 'system':
                system_instruction = msg.content
            else:
                # å…¼å®¹ï¼šå›¾åƒæ¨¡å‹ä¸‹å†å² assistant/tool æ–‡æœ¬å¯èƒ½è¦æ±‚ thoughtSignatureï¼ˆæˆ‘ä»¬æ²¡æœ‰ï¼‰ï¼Œé™çº§ä¸º user
                if msg.role == 'user':
                    role = 'user'
                else:
                    role = 'user' if self._requires_thought_signature_workaround() else 'model'
                parts = []
                
                # æ·»åŠ æ–‡æœ¬
                if msg.content:
                    if role == 'user' and msg.role != 'user' and self._requires_thought_signature_workaround():
                        parts.append(self._types.Part.from_text(text=f"[å†å²åŠ©æ‰‹æ¶ˆæ¯] {msg.content}"))
                    else:
                        parts.append(self._types.Part.from_text(text=msg.content))
                
                # æ·»åŠ åª’ä½“
                if msg.media:
                    for media_item in msg.media:
                        if not isinstance(media_item, dict):
                            continue
                        
                        media_data = media_item.get('data') or media_item.get('url', '')
                        mime_type = media_item.get('mimeType') or media_item.get('mime_type', 'image/jpeg')
                        
                        # è¿è¡Œæ—¶å…œåº•ï¼šmedia_data å¯èƒ½æ˜¯ bytes/bytearrayï¼ˆä¼šå¯¼è‡´åç»­ JSON/å¤„ç†å¤±è´¥ï¼‰
                        # - SDK è·¯å¾„ï¼šå¯ä»¥ç›´æ¥ä½¿ç”¨åŸå§‹ bytes
                        if isinstance(media_data, (bytes, bytearray)):
                            image_bytes = bytes(media_data)
                            thought_sig = media_item.get('thoughtSignature') or media_item.get('thought_signature')
                            
                            # å¦‚æœç­¾åå¼€å…³æ‰“å¼€ä¸”æ²¡æœ‰ç­¾åï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ç­¾åä½œä¸ºå‚è€ƒ
                            if not thought_sig and self.use_thoughtsig and role != 'user' and recent_thought_sig:
                                thought_sig = recent_thought_sig
                                self._log(f"ğŸ”„ ä½¿ç”¨æœ€è¿‘çš„ thoughtSignature ä½œä¸ºå‚è€ƒ ({len(thought_sig)} chars)")
                            
                            if thought_sig:
                                self._log(f"Including thoughtSignature for image ({len(thought_sig)} chars)")
                                parts.append(self._types.Part(
                                    inline_data=self._types.Blob(mime_type=mime_type, data=image_bytes),
                                    thought_signature=thought_sig
                                ))
                            else:
                                if role == 'user':
                                    parts.append(self._types.Part.from_bytes(data=image_bytes, mime_type=mime_type))
                                else:
                                    self._log("âš ï¸ Model image missing thoughtSignature, omitting image and adding placeholder text")
                                    parts.append(self._types.Part.from_text(
                                        text='[å›¾ç‰‡å·²çœç•¥ï¼šç¼ºå°‘ thoughtSignatureï¼ˆæ—§å†å²æ•°æ®ï¼‰ï¼Œæ— æ³•å‘é€ç»™ Gemini 2.5+]'
                                    ))
                            continue

                        # å¤„ç† base64 æ•°æ®
                        if isinstance(media_data, str):
                            if media_data.startswith('data:'):
                                if ';base64,' in media_data:
                                    media_data = media_data.split(';base64,', 1)[1]
                                elif ',' in media_data:
                                    media_data = media_data.split(',', 1)[1]
                            media_data = media_data.strip().replace('\n', '').replace('\r', '').replace(' ', '')
                        
                        if media_data and mime_type:
                            try:
                                image_bytes = base64.b64decode(media_data)
                                # Gemini 2.5+ï¼šå¦‚æœâ€œæŠŠæ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡â€å†å–‚å›æ¨¡å‹ï¼Œå¿…é¡»å¸¦ thought_signatureï¼›
                                # ä½†â€œç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡â€é€šå¸¸æ²¡æœ‰ thought_signatureï¼Œä»åº”å…è®¸ç”¨äºå›¾ç”Ÿå›¾ã€‚
                                thought_sig = media_item.get('thoughtSignature') or media_item.get('thought_signature')
                                
                                # å¦‚æœç­¾åå¼€å…³æ‰“å¼€ä¸”æ²¡æœ‰ç­¾åï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ç­¾åä½œä¸ºå‚è€ƒ
                                if not thought_sig and self.use_thoughtsig and role != 'user' and recent_thought_sig:
                                    thought_sig = recent_thought_sig
                                    self._log(f"ğŸ”„ ä½¿ç”¨æœ€è¿‘çš„ thoughtSignature ä½œä¸ºå‚è€ƒ ({len(thought_sig)} chars)")
                                
                                if thought_sig:
                                    # ä½¿ç”¨ SDK åŸç”Ÿ thought_signature æ”¯æŒ
                                    self._log(f"Including thoughtSignature for image ({len(thought_sig)} chars)")
                                    parts.append(self._types.Part(
                                        inline_data=self._types.Blob(mime_type=mime_type, data=image_bytes),
                                        thought_signature=thought_sig
                                    ))
                                else:
                                    if role == 'user':
                                        # ç”¨æˆ·å›¾ç‰‡ï¼šå…è®¸æ—  thoughtSignatureï¼ˆç”¨äºå›¾ç”Ÿå›¾ï¼‰
                                        parts.append(self._types.Part.from_bytes(data=image_bytes, mime_type=mime_type))
                                    else:
                                        # assistant/model å›¾ç‰‡ï¼šæ—  thoughtSignature ä¼šè§¦å‘ 400ï¼Œé™çº§ä¸ºæ–‡æœ¬å ä½
                                        self._log("âš ï¸ Model image missing thoughtSignature, omitting image and adding placeholder text")
                                        parts.append(self._types.Part.from_text(
                                            text='[å›¾ç‰‡å·²çœç•¥ï¼šç¼ºå°‘ thoughtSignatureï¼ˆæ—§å†å²æ•°æ®ï¼‰ï¼Œæ— æ³•å‘é€ç»™ Gemini 2.5+]'
                                        ))
                            except Exception as e:
                                self._log_error(f"Failed to decode image: {e}")
                
                if parts:
                    contents.append(self._types.Content(role=role, parts=parts))
        
        return contents, system_instruction
    
    def _convert_messages_for_gemini_rest(self, messages: List[LLMMessage]) -> tuple:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Gemini REST API æ ¼å¼"""
        contents = []
        system_instruction = None
        
        # æŸ¥æ‰¾æœ€è¿‘çš„ thought signatureï¼ˆç”¨äºç­¾åå¼€å…³æ‰“å¼€æ—¶çš„å‚è€ƒï¼‰
        recent_thought_sig = self._find_recent_thought_signature(messages)
        if recent_thought_sig:
            self._log(f"æ‰¾åˆ°æœ€è¿‘çš„ thoughtSignature å‚è€ƒ: {len(recent_thought_sig)} å­—ç¬¦")
        
        # é¢„æ£€æŸ¥ï¼šæ‰“å°å“ªäº›æ¶ˆæ¯åŒ…å« thoughtSignature
        self._log("=== ThoughtSignature æ£€æŸ¥ (REST) ===")
        content_idx = 0
        for i, msg in enumerate(messages):
            if msg.role == 'system':
                continue
            has_sig = False
            sig_count = 0
            if msg.media:
                for media_item in msg.media:
                    if isinstance(media_item, dict):
                        if media_item.get('thoughtSignature') or media_item.get('thought_signature'):
                            has_sig = True
                            sig_count += 1
            media_count = len(msg.media) if msg.media else 0
            status = f"âœ… {sig_count} sig" if has_sig else (f"âš ï¸ æ— sig ({media_count}å›¾)" if media_count > 0 else "")
            if media_count > 0:
                self._log(f"  Content[{content_idx}] {msg.role}: {media_count}å¼ å›¾ {status}")
            content_idx += 1
        self._log("=====================================")
        
        for msg in messages:
            if msg.role == 'system':
                system_instruction = {'parts': [{'text': msg.content}]}
            else:
                # å…¼å®¹ï¼šå›¾åƒæ¨¡å‹ä¸‹å†å² assistant/tool æ–‡æœ¬å¯èƒ½è¦æ±‚ thoughtSignatureï¼ˆæˆ‘ä»¬æ²¡æœ‰ï¼‰ï¼Œé™çº§ä¸º user
                if msg.role == 'user':
                    role = 'user'
                else:
                    role = 'user' if self._requires_thought_signature_workaround() else 'model'
                parts = []
                
                if msg.content:
                    if role == 'user' and msg.role != 'user' and self._requires_thought_signature_workaround():
                        parts.append({'text': f"[å†å²åŠ©æ‰‹æ¶ˆæ¯] {msg.content}"})
                    else:
                        parts.append({'text': msg.content})
                
                if msg.media:
                    for media_item in msg.media:
                        if not isinstance(media_item, dict):
                            continue
                        
                        media_data = media_item.get('data') or media_item.get('url', '')
                        mime_type = media_item.get('mimeType') or media_item.get('mime_type', 'image/jpeg')
                        
                        # è¿è¡Œæ—¶å…œåº•ï¼šREST payload å¿…é¡»æ˜¯ JSONï¼Œå¯åºåˆ—åŒ–ï¼›bytes/bytearray å¿…é¡»è½¬ base64 å­—ç¬¦ä¸²
                        if isinstance(media_data, (bytes, bytearray)):
                            media_data = base64.b64encode(bytes(media_data)).decode('utf-8')

                        if isinstance(media_data, str):
                            if media_data.startswith('data:'):
                                if ';base64,' in media_data:
                                    media_data = media_data.split(';base64,', 1)[1]
                                elif ',' in media_data:
                                    media_data = media_data.split(',', 1)[1]
                            media_data = media_data.strip().replace('\n', '').replace('\r', '').replace(' ', '')
                        
                        if media_data and mime_type:
                            # Gemini 2.5+ï¼šæ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡å›çŒå¿…é¡»å¸¦ thoughtSignatureï¼›ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡å¯æ— ç­¾åç”¨äºå›¾ç”Ÿå›¾ã€‚
                            thought_sig = media_item.get('thoughtSignature') or media_item.get('thought_signature')
                            
                            # å¦‚æœç­¾åå¼€å…³æ‰“å¼€ä¸”æ²¡æœ‰ç­¾åï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ç­¾åä½œä¸ºå‚è€ƒ
                            if not thought_sig and self.use_thoughtsig and role != 'user' and recent_thought_sig:
                                thought_sig = recent_thought_sig
                                self._log(f"ğŸ”„ ä½¿ç”¨æœ€è¿‘çš„ thoughtSignature ä½œä¸ºå‚è€ƒ ({len(thought_sig)} chars)")
                            
                            if thought_sig:
                                part_data = {
                                    'inlineData': {
                                        'mimeType': mime_type,
                                        'data': media_data
                                    },
                                    'thoughtSignature': thought_sig
                                }
                                self._log(f"Including thoughtSignature for image ({len(thought_sig)} chars)")
                                parts.append(part_data)
                            else:
                                if role == 'user':
                                    # ç”¨æˆ·å›¾ç‰‡ï¼šå…è®¸æ—  thoughtSignatureï¼ˆç”¨äºå›¾ç”Ÿå›¾ï¼‰
                                    parts.append({
                                        'inlineData': {
                                            'mimeType': mime_type,
                                            'data': media_data
                                        }
                                    })
                                else:
                                    # assistant/model å›¾ç‰‡ï¼šæ—  thoughtSignature ä¼šè§¦å‘ 400ï¼Œé™çº§ä¸ºæ–‡æœ¬å ä½
                                    self._log("âš ï¸ Model image missing thoughtSignature, omitting image and adding placeholder text")
                                    parts.append({'text': '[å›¾ç‰‡å·²çœç•¥ï¼šç¼ºå°‘ thoughtSignatureï¼ˆæ—§å†å²æ•°æ®ï¼‰ï¼Œæ— æ³•å‘é€ç»™ Gemini 2.5+]'})
                
                if parts:
                    contents.append({'role': role, 'parts': parts})
        
        return contents, system_instruction
    
    def _parse_error_response(self, response) -> str:
        """è§£æé”™è¯¯å“åº”"""
        try:
            error_data = response.json()
            msg = (error_data.get('error', {}) or {}).get('message', '')
            if msg:
                return msg
            if response.text:
                return response.text
            return f"HTTP {response.status_code}"
        except:
            return response.text or f"HTTP {response.status_code}"
    
    def models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        ä¼˜å…ˆä½¿ç”¨ SDKï¼Œå›é€€åˆ° REST API
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ SDK
            if self.sdk_available and self._client:
                try:
                    # Google GenAI SDK å¯èƒ½æ²¡æœ‰ç›´æ¥çš„ models.list() æ–¹æ³•
                    # å°è¯•ä½¿ç”¨ REST API
                    pass
                except Exception as e:
                    self._log(f"SDK models() not available: {e}, using REST API")
            
            # ä½¿ç”¨ REST API
            # Gemini API: https://generativelanguage.googleapis.com/v1beta/models?key=API_KEY
            base_url = self.api_url or 'https://generativelanguage.googleapis.com'
            if '/v1beta' not in base_url and '/v1' not in base_url:
                base_url = f"{base_url.rstrip('/')}/v1beta"
            elif base_url.endswith('/v1'):
                base_url = base_url.replace('/v1', '/v1beta')
            
            models_url = f"{base_url}/models"
            params = {'key': self.api_key}
            
            self._log(f"Fetching models via REST API: {models_url}")
            response = requests.get(models_url, params=params, timeout=10)
            
            if response.status_code != 200:
                raise RuntimeError(f"Failed to fetch models: {response.status_code} {response.text}")
            
            data = response.json()
            # Gemini è¿”å›æ ¼å¼ï¼š{ models: [{ name: "...", ... }] }
            if isinstance(data, dict) and isinstance(data.get('models'), list):
                model_names = [model.get('name') for model in data['models'] if model.get('name')]
                # æå–æ¨¡å‹ IDï¼ˆä»å®Œæ•´åç§°ä¸­ï¼Œå¦‚ "models/gemini-2.0-flash-exp" -> "gemini-2.0-flash-exp"ï¼‰
                model_ids = []
                for name in model_names:
                    if '/' in name:
                        model_ids.append(name.split('/')[-1])
                    else:
                        model_ids.append(name)
                self._log(f"Fetched {len(model_ids)} models via REST API")
                return model_ids
            
            # å…¼å®¹å…¶ä»–æ ¼å¼
            if isinstance(data, list):
                model_ids = [item.get('name') if isinstance(item, dict) else item for item in data if item]
                # æå–æ¨¡å‹ ID
                extracted_ids = []
                for name in model_ids:
                    if isinstance(name, str) and '/' in name:
                        extracted_ids.append(name.split('/')[-1])
                    elif name:
                        extracted_ids.append(name)
                self._log(f"Fetched {len(extracted_ids)} models via REST API (array format)")
                return extracted_ids
            
            raise RuntimeError("Invalid response format from models API")
            
        except Exception as e:
            self._log_error(f"Failed to fetch models: {e}", e)
            raise

    def list_models(self) -> List[Dict[str, Any]]:
        """
        è·å–æ¨¡å‹åˆ—è¡¨åŠå¯è°ƒç”¨æ€§ä¿¡æ¯ï¼ˆç”¨äºèŠå¤© generateContentï¼‰ã€‚
        è§£æ Google API è¿”å›çš„ supportedGenerationMethodsï¼Œä»…æ”¯æŒ generateContent çš„æ¨¡å‹è§†ä¸ºå¯å¯¹è¯ã€‚
        ç»“æœå¸¦ç®€å•å†…å­˜ç¼“å­˜ï¼ŒTTL 5 åˆ†é’Ÿã€‚
        """
        cache_key = (self.api_url or '', self.api_key or '')
        now = time.time()
        if cache_key in _LIST_MODELS_CACHE:
            ts, cached = _LIST_MODELS_CACHE[cache_key]
            if now - ts < _LIST_MODELS_CACHE_TTL:
                self._log(f"Using cached list_models ({len(cached)} models)")
                return cached
        base_url = self.api_url or 'https://generativelanguage.googleapis.com'
        if '/v1beta' not in base_url and '/v1' not in base_url:
            base_url = f"{base_url.rstrip('/')}/v1beta"
        elif base_url.endswith('/v1'):
            base_url = base_url.replace('/v1', '/v1beta')
        models_url = f"{base_url}/models"
        params = {'key': self.api_key}
        self._log(f"Fetching list_models via REST: {models_url}")
        response = requests.get(models_url, params=params, timeout=10)
        if response.status_code != 200:
            raise RuntimeError(f"Failed to fetch models: {response.status_code} {response.text}")
        data = response.json()
        result = []
        if not isinstance(data, dict) or not isinstance(data.get('models'), list):
            raise RuntimeError("Invalid response format from models API")
        for model in data['models']:
            name = model.get('name')
            if not name or not isinstance(name, str):
                continue
            model_id = name.split('/')[-1] if '/' in name else name
            methods = model.get('supportedGenerationMethods') or model.get('supported_generation_methods') or []
            if isinstance(methods, str):
                methods = [methods]
            is_callable = 'generateContent' in methods
            result.append({
                'id': model_id,
                'is_callable': is_callable,
                'supported_generation_methods': list(methods),
            })
        _LIST_MODELS_CACHE[cache_key] = (now, result)
        self._log(f"list_models: {len(result)} models, {sum(1 for r in result if r['is_callable'])} callable for chat")
        return result
