"""
çˆ¬è™«æ•°æ®æ ‡å‡†åŒ–æ¨¡å—
å°†çˆ¬å–ç»“æœæ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œä¾¿äºåç»­å¤„ç†å’Œå¼•ç”¨
"""

import re
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime


class CrawlerNormalizer:
    """æ•°æ®æ ‡å‡†åŒ–å™¨"""
    
    def normalize(self, raw_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        æ ‡å‡†åŒ–çˆ¬å–ç»“æœ
        
        Args:
            raw_data: åŸå§‹çˆ¬å–ç»“æœ
            config: æ ‡å‡†åŒ–é…ç½®
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        if not raw_data.get('success'):
            return raw_data
        
        if not config:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼
            return self._auto_detect_format(raw_data)
        
        format_type = config.get('format', 'article')
        
        if format_type == 'list':
            return self._normalize_list(raw_data, config)
        elif format_type == 'table':
            return self._normalize_table(raw_data, config)
        elif format_type == 'article':
            return self._normalize_article(raw_data, config)
        else:
            return self._normalize_custom(raw_data, config)
    
    def _auto_detect_format(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è‡ªåŠ¨æ£€æµ‹æ•°æ®æ ¼å¼
        å°è¯•è¯†åˆ«åˆ—è¡¨ã€è¡¨æ ¼ç­‰ç»“æ„åŒ–æ•°æ®
        """
        html = raw_data.get('content', {}).get('html', '')
        if not html:
            return self._normalize_default(raw_data)
        
        soup = BeautifulSoup(html, 'lxml')
        
        # 1. æ£€æµ‹è¡¨æ ¼
        tables = soup.find_all('table')
        if tables:
            # æ‰¾åˆ°æœ€å¤§çš„è¡¨æ ¼
            largest_table = max(tables, key=lambda t: len(t.find_all('tr')))
            if len(largest_table.find_all('tr')) >= 2:  # è‡³å°‘2è¡Œï¼ˆè¡¨å¤´+æ•°æ®ï¼‰
                return self._normalize_table(raw_data, {
                    'format': 'table',
                    'table_selector': 'table',
                    'header_row': 0
                })
        
        # 2. æ£€æµ‹åˆ—è¡¨ç»“æ„ï¼ˆå¸¸è§çš„åˆ—è¡¨é€‰æ‹©å™¨ï¼‰
        list_selectors = [
            'ul > li',
            'ol > li',
            '.list-item',
            '.item',
            '.article-item',
            '.post-item',
            '.news-item',
            '[class*="item"]',
            '[class*="list"]',
        ]
        
        for selector in list_selectors:
            items = soup.select(selector)
            if len(items) >= 3:  # è‡³å°‘3ä¸ªåˆ—è¡¨é¡¹
                # å°è¯•æå–æ ‡é¢˜å’Œå†…å®¹
                first_item = items[0]
                # æŸ¥æ‰¾æ ‡é¢˜å…ƒç´ 
                title_elem = None
                for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    title_elem = first_item.find(tag)
                    if title_elem:
                        break
                if not title_elem:
                    title_elem = first_item.find(class_=lambda x: x and 'title' in str(x).lower())
                
                # æŸ¥æ‰¾å†…å®¹å…ƒç´ 
                content_elem = first_item.find('p')
                if not content_elem:
                    content_elem = first_item.find(class_=lambda x: x and ('content' in str(x).lower() or 'description' in str(x).lower()))
                
                if title_elem or content_elem:
                    # æ‰¾åˆ°äº†åˆ—è¡¨ç»“æ„
                    return self._normalize_list(raw_data, {
                        'format': 'list',
                        'item_selector': selector.split(' > ')[0] if ' > ' in selector else selector,
                        'title_selector': 'h1, h2, h3, h4, h5, h6, .title',
                        'content_selector': 'p, .content, .description'
                    })
        
        # 3. æ£€æµ‹é‡å¤çš„divç»“æ„ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨ï¼‰
        def has_item_class(class_name):
            if not class_name:
                return False
            if isinstance(class_name, list):
                class_name = ' '.join(class_name)
            return 'item' in class_name.lower() or 'card' in class_name.lower() or 'post' in class_name.lower()
        
        divs = soup.find_all('div', class_=has_item_class)
        if len(divs) >= 3:
            # æ£€æŸ¥è¿™äº›divæ˜¯å¦æœ‰ç›¸ä¼¼çš„ç»“æ„
            def has_title_elem(elem):
                if elem.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                    return True
                title_elem = elem.find(class_=lambda x: x and 'title' in str(x).lower())
                return title_elem is not None
            
            has_title = sum(1 for d in divs[:5] if has_title_elem(d))
            if has_title >= 2:  # è‡³å°‘2ä¸ªæœ‰æ ‡é¢˜
                return self._normalize_list(raw_data, {
                    'format': 'list',
                    'item_selector': 'div[class*="item"], div[class*="card"], div[class*="post"]',
                    'title_selector': 'h1, h2, h3, h4, h5, h6, [class*="title"]',
                    'content_selector': 'p, [class*="content"], [class*="description"]'
                })
        
        # 4. æ£€æµ‹é“¾æ¥åˆ—è¡¨ï¼ˆå¦‚æœé“¾æ¥æ•°é‡å¾ˆå¤šï¼Œå¯èƒ½æ˜¯åˆ—è¡¨é¡µé¢ï¼‰
        links = soup.find_all('a', href=True)
        if len(links) >= 10:  # è‡³å°‘10ä¸ªé“¾æ¥
            # å°è¯•æ‰¾åˆ°é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œçœ‹æ˜¯å¦æœ‰é‡å¤ç»“æ„
            link_parents = {}
            for link in links[:20]:  # æ£€æŸ¥å‰20ä¸ªé“¾æ¥
                parent = link.parent
                if parent:
                    parent_tag = parent.name
                    parent_class = parent.get('class', [])
                    parent_id = parent.get('id', '')
                    # ç”Ÿæˆçˆ¶å…ƒç´ çš„ç‰¹å¾
                    key = f"{parent_tag}_{'_'.join(sorted(parent_class))}_{parent_id}"
                    if key not in link_parents:
                        link_parents[key] = []
                    link_parents[key].append(link)
            
            # æ‰¾åˆ°åŒ…å«æœ€å¤šé“¾æ¥çš„çˆ¶å®¹å™¨
            if link_parents:
                best_parent_key = max(link_parents.keys(), key=lambda k: len(link_parents[k]))
                best_parent_links = link_parents[best_parent_key]
                
                if len(best_parent_links) >= 5:  # è‡³å°‘5ä¸ªé“¾æ¥åœ¨åŒä¸€ç±»å‹çš„çˆ¶å®¹å™¨ä¸­
                    # è·å–ç¬¬ä¸€ä¸ªé“¾æ¥çš„çˆ¶å®¹å™¨ä½œä¸ºç¤ºä¾‹
                    sample_parent = best_parent_links[0].parent
                    parent_tag = sample_parent.name
                    parent_class = sample_parent.get('class', [])
                    
                    # æ„å»ºé€‰æ‹©å™¨
                    if parent_class:
                        class_selector = '.'.join([c for c in parent_class if c])
                        item_selector = f"{parent_tag}.{class_selector}"
                    else:
                        item_selector = parent_tag
                    
                    # å°è¯•æå–ï¼šé“¾æ¥æ–‡æœ¬ä½œä¸ºæ ‡é¢˜ï¼Œé“¾æ¥çš„å…„å¼Ÿå…ƒç´ ä½œä¸ºå†…å®¹
                    return self._normalize_list(raw_data, {
                        'format': 'list',
                        'item_selector': item_selector,
                        'title_selector': 'a',
                        'content_selector': 'p, span, div'
                    })
        
        # 5. æ£€æµ‹é‡å¤çš„é“¾æ¥ç»“æ„ï¼ˆé“¾æ¥åœ¨åŒä¸€å±‚çº§çš„å®¹å™¨ä¸­ï¼‰
        # æ‰¾åˆ°åŒ…å«å¤šä¸ªé“¾æ¥çš„å®¹å™¨
        containers_with_links = []
        for container in soup.find_all(['div', 'section', 'article', 'ul', 'ol']):
            container_links = container.find_all('a', href=True)
            if len(container_links) >= 3:
                containers_with_links.append((container, len(container_links)))
        
        if containers_with_links:
            # æŒ‰é“¾æ¥æ•°é‡æ’åº
            containers_with_links.sort(key=lambda x: x[1], reverse=True)
            best_container, link_count = containers_with_links[0]
            
            # æ£€æŸ¥å®¹å™¨å†…çš„é“¾æ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„ç»“æ„ï¼ˆæ¯”å¦‚éƒ½åœ¨liä¸­ï¼Œæˆ–è€…éƒ½åœ¨ç‰¹å®šçš„divä¸­ï¼‰
            # æ‰¾åˆ°é“¾æ¥çš„ç›´æ¥çˆ¶å…ƒç´ 
            link_parents_set = set()
            for link in best_container.find_all('a', href=True)[:10]:
                parent = link.parent
                if parent:
                    parent_tag = parent.name
                    parent_class = parent.get('class', [])
                    if isinstance(parent_class, list):
                        parent_class = ' '.join(parent_class)
                    link_parents_set.add((parent_tag, parent_class))
            
            # å¦‚æœå¤§éƒ¨åˆ†é“¾æ¥çš„çˆ¶å…ƒç´ ç±»å‹ç›¸åŒï¼Œè¯´æ˜æ˜¯åˆ—è¡¨ç»“æ„
            if len(link_parents_set) <= 2:  # æœ€å¤š2ç§çˆ¶å…ƒç´ ç±»å‹
                # ä½¿ç”¨æœ€å¸¸è§çš„çˆ¶å…ƒç´ ä½œä¸ºitemé€‰æ‹©å™¨
                parent_tag, parent_class = list(link_parents_set)[0]
                if parent_class:
                    item_selector = f"{parent_tag}.{parent_class.replace(' ', '.')}"
                else:
                    item_selector = parent_tag
                
                return self._normalize_list(raw_data, {
                    'format': 'list',
                    'item_selector': item_selector,
                    'title_selector': 'a',
                    'content_selector': 'p, span, div'
                })
        
        # 6. é»˜è®¤ä½œä¸ºæ–‡ç« å¤„ç†
        return self._normalize_default(raw_data)
    
    def _normalize_default(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """é»˜è®¤æ ‡å‡†åŒ–ï¼ˆæ•´ç¯‡æ–‡ç« ä½œä¸ºä¸€ä¸ªitemï¼‰"""
        return {
            **raw_data,
            'normalized': {
                'format': 'article',
                'items': [{
                    'id': 'item_1',
                    'title': raw_data.get('title', ''),
                    'content': raw_data.get('content', {}).get('text', ''),
                    'html': raw_data.get('content', {}).get('html', ''),
                    'metadata': raw_data.get('metadata', {}),
                    'images': raw_data.get('images', []),
                    'links': raw_data.get('links', []),
                    'extracted_at': datetime.now().isoformat()
                }],
                'total_count': 1
            }
        }
    
    def _normalize_list(self, raw_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ—è¡¨æ ¼å¼æ ‡å‡†åŒ–
        
        é…ç½®ç¤ºä¾‹:
        {
            "format": "list",
            "item_selector": ".article-item",
            "title_selector": "h2.title",
            "content_selector": ".content",
            "metadata_selectors": {
                "author": ".author",
                "date": ".date"
            }
        }
        """
        html = raw_data.get('content', {}).get('html', '')
        if not html:
            return self._normalize_default(raw_data)
        
        soup = BeautifulSoup(html, 'lxml')
        item_selector = config.get('item_selector', '.item')
        
        # è·å–é€‰æ‹©å™¨ï¼Œå¦‚æœæ˜ç¡®ä¼ é€’äº†ç©ºå­—ç¬¦ä¸²ï¼Œä¸ä½¿ç”¨é»˜è®¤å€¼
        title_selector = config.get('title_selector')
        if title_selector is None:
            title_selector = 'h2, h3, .title'  # åªæœ‰æœªä¼ é€’æ—¶æ‰ç”¨é»˜è®¤å€¼
        
        content_selector = config.get('content_selector')
        if content_selector is None:
            content_selector = '.content, p'  # åªæœ‰æœªä¼ é€’æ—¶æ‰ç”¨é»˜è®¤å€¼
        
        metadata_selectors = config.get('metadata_selectors', {})
        
        items = []
        item_elements = soup.select(item_selector)
        
        print(f"[Normalizer] Found {len(item_elements)} items using selector '{item_selector}'")
        print(f"[Normalizer] Selectors - title: '{title_selector or 'None'}', content: '{content_selector or 'None'}'")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé€‰æ‹©å™¨ï¼Œç›´æ¥æå–æ¯ä¸ªitemçš„æ‰€æœ‰æ–‡æœ¬ä½œä¸ºå¿«ç…§
        if not title_selector and not content_selector:
            print(f"[Normalizer] ğŸš€ ç®€åŒ–æ¨¡å¼ï¼šæ²¡æœ‰æŒ‡å®šé€‰æ‹©å™¨ï¼Œç›´æ¥æå–çº¯æ–‡æœ¬å¿«ç…§")
            
            # è·å–åˆ†å‰²è§„åˆ™
            split_pattern = config.get('split_pattern', '')  # ç”¨äºåˆ†å‰²å¤šä¸ªæ•°æ®é¡¹çš„æ¨¡å¼
            split_strategy = config.get('split_strategy', 'none')  # none, regex, keyword
            
            print(f"[Normalizer] æ•°æ®é¡¹åˆ†å‰²ç­–ç•¥: {split_strategy}, åˆ†å‰²æ¨¡å¼: '{split_pattern}'")
            
            item_counter = 0
            
            for idx, item_elem in enumerate(item_elements, 1):
                # ç›´æ¥æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                full_text = item_elem.get_text(separator='\n', strip=True)
                
                # æ¸…ç†æ–‡æœ¬
                lines = [line.strip() for line in full_text.split('\n') if line.strip()]
                full_text = '\n'.join(lines)
                
                print(f"[Normalizer] å¤„ç†ç¬¬ {idx} ä¸ªå…ƒç´ ï¼ŒåŸå§‹æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
                
                # ç¬¬ä¸€æ­¥ï¼šåˆ†å‰²æˆå¤šä¸ªå­é¡¹
                sub_items = []
                
                if split_strategy == 'regex' and split_pattern:
                    try:
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æˆå¤šä¸ªå­é¡¹
                        parts = re.split(split_pattern, full_text)
                        sub_items = [p.strip() for p in parts if p.strip()]
                        print(f"[Normalizer]   æ­£åˆ™åˆ†å‰²: {len(sub_items)} ä¸ªå­é¡¹")
                    except re.error as e:
                        print(f"[Normalizer]   æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}ï¼Œä¸åˆ†å‰²")
                        sub_items = [full_text]
                
                elif split_strategy == 'keyword' and split_pattern:
                    # ä½¿ç”¨å…³é”®è¯åˆ†å‰²æˆå¤šä¸ªå­é¡¹
                    if split_pattern in full_text:
                        parts = full_text.split(split_pattern)
                        # ä¿ç•™åˆ†éš”ç¬¦ï¼Œå°†å…¶æ·»åŠ åˆ°æ¯ä¸ªéƒ¨åˆ†çš„å¼€å¤´ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªï¼‰
                        sub_items = []
                        for i, part in enumerate(parts):
                            if part.strip():
                                if i > 0:
                                    # ç¬¬äºŒä¸ªåŠä»¥åçš„éƒ¨åˆ†ï¼Œæ·»åŠ åˆ†éš”ç¬¦
                                    sub_items.append(split_pattern + '\n' + part.strip())
                                else:
                                    sub_items.append(part.strip())
                        print(f"[Normalizer]   å…³é”®è¯åˆ†å‰²: {len(sub_items)} ä¸ªå­é¡¹ï¼ˆå…³é”®è¯: '{split_pattern}'ï¼‰")
                    else:
                        sub_items = [full_text]
                        print(f"[Normalizer]   æœªæ‰¾åˆ°å…³é”®è¯ '{split_pattern}'ï¼Œä¿æŒä¸º 1 ä¸ªå­é¡¹")
                
                else:
                    # ä¸åˆ†å‰²ï¼Œæ•´ä¸ªä½œä¸ºä¸€ä¸ªå­é¡¹
                    sub_items = [full_text]
                    print(f"[Normalizer]   ä¸åˆ†å‰²ï¼Œä¿æŒä¸º 1 ä¸ªå­é¡¹")
                
                # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªå­é¡¹æå– title å’Œ content
                for sub_idx, sub_text in enumerate(sub_items, 1):
                    item_counter += 1
                    
                    sub_lines = [line.strip() for line in sub_text.split('\n') if line.strip()]
                    
                    # é»˜è®¤ä½¿ç”¨é¦–è¡Œæ¨¡å¼æå– title
                    title = ''
                    content = sub_text
                    
                    if len(sub_lines) > 0:
                        first_line = sub_lines[0]
                        # å¦‚æœç¬¬ä¸€è¡ŒçŸ­äº100å­—ç¬¦ï¼Œä½œä¸ºæ ‡é¢˜
                        if len(first_line) < 100:
                            title = first_line
                            if len(sub_lines) > 1:
                                content = '\n'.join(sub_lines[1:])
                            else:
                                content = ''
                        else:
                            # ç¬¬ä¸€è¡Œå¤ªé•¿ï¼Œæ•´ä¸ªä½œä¸ºå†…å®¹
                            title = ''
                            content = sub_text
                    
                    print(f"[Normalizer]     â†’ å­é¡¹ {sub_idx}: title='{title[:30]}...', title_len={len(title)}, content_len={len(content)}")
                    
                    items.append({
                        'id': f'item_{item_counter}',
                        'title': title,
                        'content': content,
                        'text': sub_text,
                        'html': str(item_elem) if sub_idx == 1 else '',  # åªæœ‰ç¬¬ä¸€ä¸ªå­é¡¹ä¿å­˜HTML
                        'metadata': {
                            'source_element_index': idx,
                            'sub_item_index': sub_idx
                        },
                        'extracted_at': datetime.now().isoformat()
                    })
            
            print(f"[Normalizer] âœ… ç®€åŒ–æ¨¡å¼å®Œæˆï¼Œä» {len(item_elements)} ä¸ªå…ƒç´ æå–äº† {len(items)} ä¸ªæ•°æ®é¡¹")
            return {
                **raw_data,
                'normalized': {
                    'format': 'list',
                    'items': items,
                    'total_count': len(items),
                    'extraction_info': {
                        'method': 'text_snapshot',
                        'note': 'Direct text extraction without selectors'
                    }
                }
            }
        
        # ç»Ÿè®¡é€‰æ‹©å™¨åŒ¹é…æƒ…å†µ
        matched_titles = 0
        matched_contents = 0
        fallback_titles = 0
        fallback_contents = 0
        
        for idx, item_elem in enumerate(item_elements, 1):
            # æå–æ ‡é¢˜
            title = ''
            title_found = False
            if title_selector:
                title_elem = item_elem.select_one(title_selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    title_found = True
                    matched_titles += 1
                    print(f"[Normalizer] Item {idx}: Found title using selector '{title_selector}': {title[:50]}")
                else:
                    print(f"[Normalizer] Item {idx}: âš ï¸ Title selector '{title_selector}' did not match any element, will try fallback methods")
            
            # å¦‚æœæ ‡é¢˜é€‰æ‹©å™¨æ˜¯ 'a'ï¼Œæå–é“¾æ¥æ–‡æœ¬å’ŒURL
            if title_selector == 'a' and not title:
                link_elem = item_elem.find('a', href=True)
                if link_elem:
                    title = link_elem.get_text(strip=True)
                    title_found = True
                    print(f"[Normalizer] Item {idx}: Found title from link: {title[:50]}")
            
            # å¦‚æœæ ‡é¢˜ä»ç„¶ä¸ºç©ºï¼Œå°è¯•ä» item ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜å…ƒç´ ï¼ˆh1-h6ï¼‰
            if not title.strip():
                for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    heading = item_elem.find(tag)
                    if heading:
                        title = heading.get_text(strip=True)
                        fallback_titles += 1
                        print(f"[Normalizer] Item {idx}: âœ… Fallback - Found title from <{tag}> tag: {title[:50]}")
                        break
            
            # æå–å†…å®¹
            content = ''
            content_html = ''
            if content_selector:
                content_elem = item_elem.select_one(content_selector)
                if content_elem:
                    matched_contents += 1
                    print(f"[Normalizer] Item {idx}: Found content element using selector '{content_selector}', tag: {content_elem.name}")
                    # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ŒåŒ…æ‹¬é“¾æ¥å†…çš„æ–‡æœ¬
                    # ä½¿ç”¨ get_text ä¼šé€’å½’æå–æ‰€æœ‰å­å…ƒç´ çš„æ–‡æœ¬
                    # å¦‚æœé€‰æ‹©å™¨é€‰æ‹©åˆ°çš„æ˜¯é“¾æ¥ï¼Œéœ€è¦æå–é“¾æ¥å†…çš„æ‰€æœ‰æ–‡æœ¬ï¼Œè€Œä¸ä»…ä»…æ˜¯é“¾æ¥æ–‡æœ¬
                    if content_elem.name == 'a':
                        # å¦‚æœæ˜¯é“¾æ¥å…ƒç´ ï¼Œæå–é“¾æ¥å†…çš„æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬åµŒå¥—å…ƒç´ ï¼‰
                        content = content_elem.get_text(separator='\n', strip=True)
                        print(f"[Normalizer] Item {idx}: Extracted content from link, length: {len(content)}")
                        # å¦‚æœé“¾æ¥å†…æ²¡æœ‰æ–‡æœ¬ï¼Œå°è¯•è·å–é“¾æ¥çš„ title æˆ– href
                        if not content.strip():
                            content = content_elem.get('title', '') or content_elem.get('href', '')
                            print(f"[Normalizer] Item {idx}: Link has no text, using title/href: {content[:50]}")
                    else:
                        # æ™®é€šå…ƒç´ ï¼Œæå–æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬é“¾æ¥å†…çš„æ–‡æœ¬ï¼‰
                        content = content_elem.get_text(separator='\n', strip=True)
                        print(f"[Normalizer] Item {idx}: Extracted content from element, length: {len(content)}, preview: {content[:100]}")
                    content_html = str(content_elem)
                else:
                    print(f"[Normalizer] Item {idx}: âš ï¸ Content selector '{content_selector}' did not match any element, will try fallback methods")
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šå†…å®¹é€‰æ‹©å™¨ï¼Œä½¿ç”¨æ•´ä¸ªitemçš„æ–‡æœ¬ï¼ˆæ’é™¤æ ‡é¢˜ï¼‰
                print(f"[Normalizer] Item {idx}: No content selector, using entire item text")
                # å…ˆç§»é™¤æ ‡é¢˜å…ƒç´ 
                item_copy = BeautifulSoup(str(item_elem), 'lxml')
                if title_selector:
                    for title_elem in item_copy.select(title_selector):
                        title_elem.decompose()
                # æå–æ‰€æœ‰æ–‡æœ¬ï¼ŒåŒ…æ‹¬é“¾æ¥å†…çš„æ–‡æœ¬
                content = item_copy.get_text(separator='\n', strip=True)
                content_html = str(item_copy)
                print(f"[Normalizer] Item {idx}: Extracted content from entire item (excluding title), length: {len(content)}")
            
            # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œå°è¯•è·å–itemçš„æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬é“¾æ¥å†…çš„æ–‡æœ¬ï¼‰
            if not content.strip():
                fallback_contents += 1
                print(f"[Normalizer] Item {idx}: âš ï¸ Content is empty after selector extraction, using fallback: extract from entire item")
                # è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ŒåŒ…æ‹¬é“¾æ¥å†…çš„æ–‡æœ¬
                content = item_elem.get_text(separator='\n', strip=True)
                content_html = str(item_elem)
                print(f"[Normalizer] Item {idx}: âœ… Fallback succeeded - Extracted content from entire item, length: {len(content)}")
                print(f"[Normalizer] Item {idx}: Item HTML structure: {str(item_elem)[:300]}")
                
                # å¦‚æœè¿˜æ˜¯ä¸ºç©ºï¼Œè¾“å‡ºè­¦å‘Š
                if not content.strip():
                    print(f"[Normalizer] Item {idx}: âŒ ERROR: Content is still empty after all fallback attempts!")
                    print(f"[Normalizer] Item {idx}: Item HTML preview: {str(item_elem)[:200]}")
            
            # æ¸…ç†å†…å®¹ï¼šç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œå’Œç©ºæ ¼
            if content:
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                content = '\n'.join(lines)
            
            # æ™ºèƒ½å¤„ç†ï¼šå¦‚æœæ ‡é¢˜ä¸ºç©ºä½†å†…å®¹ä¸ä¸ºç©ºï¼Œå°è¯•å°†å†…å®¹çš„ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
            # è¿™é€šå¸¸å‘ç”Ÿåœ¨ç”¨æˆ·æ²¡æœ‰æ ‡è®°æ ‡é¢˜é€‰æ‹©å™¨çš„æƒ…å†µ
            # é‡è¦ï¼šä¿ç•™åŸå§‹ contentï¼Œä¸è¦æ¸…ç©ºï¼
            original_content = content  # ä¿å­˜åŸå§‹å†…å®¹
            if not title.strip() and content.strip():
                content_lines = content.split('\n')
                if len(content_lines) > 0:
                    # å¦‚æœå†…å®¹åªæœ‰ä¸€è¡Œï¼Œæ•´è¡Œä½œä¸ºæ ‡é¢˜ï¼ŒåŒæ—¶ä¹Ÿä¿ç•™åœ¨ content ä¸­
                    if len(content_lines) == 1:
                        title = content_lines[0].strip()
                        # ä¿ç•™åŸå§‹å†…å®¹ï¼Œä¸æ¸…ç©ºï¼
                        content = original_content
                        print(f"[Normalizer] Item {idx}: Content has only one line, using as both title and content: {title[:50]}")
                    # å¦‚æœå†…å®¹æœ‰å¤šè¡Œï¼Œç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                    elif len(content_lines[0]) < 100:  # æ ‡é¢˜é€šå¸¸è¾ƒçŸ­ï¼ˆå°‘äº100å­—ç¬¦ï¼‰
                        title = content_lines[0].strip()
                        # å¯é€‰ï¼šä¿ç•™å®Œæ•´å†…å®¹ï¼ˆåŒ…æ‹¬æ ‡é¢˜è¡Œï¼‰æˆ–ç§»é™¤æ ‡é¢˜è¡Œ
                        # è¿™é‡Œä¿ç•™å®Œæ•´å†…å®¹æ›´å®‰å…¨
                        content = original_content
                        print(f"[Normalizer] Item {idx}: Using first line as title: {title[:50]}, keeping full content length: {len(content)}")
            
            print(f"[Normalizer] Item {idx}: Final - title_length={len(title)}, content_length={len(content)}")
            
            # æå–å…ƒæ•°æ®
            metadata = {}
            for key, selector in metadata_selectors.items():
                meta_elem = item_elem.select_one(selector)
                if meta_elem:
                    metadata[key] = meta_elem.get_text(strip=True)
            
            # æå–é“¾æ¥
            link_elem = item_elem.find('a', href=True)
            if link_elem:
                link_url = link_elem.get('href', '')
                if link_url:
                    metadata['url'] = link_url
                    # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨é“¾æ¥æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
                    if not title:
                        title = link_elem.get_text(strip=True)
            
            # åªæœ‰å½“æ ‡é¢˜æˆ–å†…å®¹ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ 
            if title or content.strip():
                items.append({
                    'id': f"item_{idx}",
                    'title': title,
                    'content': content,
                    'html': content_html,
                    'metadata': metadata,
                    'extracted_at': datetime.now().isoformat()
                })
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°itemsï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åŒ–
        if not items:
            return self._normalize_default(raw_data)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n[Normalizer] ========== Extraction Summary ==========")
        print(f"[Normalizer] Total items: {len(items)}")
        print(f"[Normalizer] Title - Matched: {matched_titles}, Fallback: {fallback_titles}, Failed: {len(items) - matched_titles - fallback_titles}")
        print(f"[Normalizer] Content - Matched: {matched_contents}, Fallback: {fallback_contents}, Failed: {len(items) - matched_contents - fallback_contents}")
        if matched_titles < len(items) * 0.5:
            print(f"[Normalizer] âš ï¸ WARNING: Less than 50% of items matched title selector, consider updating selector")
        if matched_contents < len(items) * 0.5:
            print(f"[Normalizer] âš ï¸ WARNING: Less than 50% of items matched content selector, consider updating selector")
        print(f"[Normalizer] ==========================================\n")
        
        return {
            **raw_data,
            'normalized': {
                'format': 'list',
                'items': items,
                'total_count': len(items),
                'extraction_info': {
                    'method': 'selector',
                    'selectors_used': [item_selector, title_selector, content_selector],
                    'match_stats': {
                        'total': len(items),
                        'title_matched': matched_titles,
                        'title_fallback': fallback_titles,
                        'content_matched': matched_contents,
                        'content_fallback': fallback_contents
                    }
                }
            }
        }
    
    def _normalize_table(self, raw_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¡¨æ ¼æ ¼å¼æ ‡å‡†åŒ–
        
        é…ç½®ç¤ºä¾‹:
        {
            "format": "table",
            "table_selector": "table",
            "header_row": 0,  # è¡¨å¤´è¡Œç´¢å¼•
            "skip_rows": []   # è·³è¿‡çš„è¡Œç´¢å¼•
        }
        """
        html = raw_data.get('content', {}).get('html', '')
        if not html:
            return self._normalize_default(raw_data)
        
        soup = BeautifulSoup(html, 'lxml')
        table_selector = config.get('table_selector', 'table')
        header_row = config.get('header_row', 0)
        skip_rows = config.get('skip_rows', [])
        
        table = soup.select_one(table_selector)
        if not table:
            return self._normalize_default(raw_data)
        
        rows = table.find_all('tr')
        if not rows:
            return self._normalize_default(raw_data)
        
        # æå–è¡¨å¤´
        headers = []
        if header_row is not None and header_row < len(rows):
            header_row_elem = rows[header_row]
            headers = [th.get_text(strip=True) for th in header_row_elem.find_all(['th', 'td'])]
        
        # æå–æ•°æ®è¡Œ
        items = []
        for idx, row in enumerate(rows):
            if idx == header_row or idx in skip_rows:
                continue
            
            cells = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]
            if not cells:
                continue
            
            # æ„å»ºæ•°æ®é¡¹
            item = {
                'id': f"item_{len(items) + 1}",
                'content': ' | '.join(cells),  # è¡¨æ ¼è¡Œå†…å®¹
                'metadata': {}
            }
            
            # å¦‚æœæœ‰è¡¨å¤´ï¼Œå°†æ•°æ®æ˜ å°„åˆ°è¡¨å¤´
            if headers:
                for i, header in enumerate(headers):
                    if i < len(cells):
                        item['metadata'][header] = cells[i]
            else:
                # æ²¡æœ‰è¡¨å¤´ï¼Œä½¿ç”¨åˆ—ç´¢å¼•
                for i, cell in enumerate(cells):
                    item['metadata'][f'column_{i}'] = cell
            
            item['extracted_at'] = datetime.now().isoformat()
            items.append(item)
        
        if not items:
            return self._normalize_default(raw_data)
        
        return {
            **raw_data,
            'normalized': {
                'format': 'table',
                'items': items,
                'total_count': len(items),
                'headers': headers,
                'extraction_info': {
                    'method': 'table',
                    'table_selector': table_selector
                }
            }
        }
    
    def _normalize_article(self, raw_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ–‡ç« æ ¼å¼æ ‡å‡†åŒ–ï¼ˆä¸é»˜è®¤ç±»ä¼¼ï¼Œä½†å¯ä»¥é…ç½®æ›´ç²¾ç»†çš„é€‰æ‹©å™¨ï¼‰
        """
        # æ–‡ç« æ ¼å¼é€šå¸¸å°±æ˜¯æ•´ç¯‡æ–‡ç« ï¼Œä¸é»˜è®¤æ ‡å‡†åŒ–ç±»ä¼¼
        return self._normalize_default(raw_data)
    
    def _normalize_custom(self, raw_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è‡ªå®šä¹‰æ ¼å¼æ ‡å‡†åŒ–
        
        é…ç½®ç¤ºä¾‹:
        {
            "format": "custom",
            "custom_extractors": {
                "price": {
                    "selector": ".price",
                    "type": "number"
                },
                "rating": {
                    "selector": ".rating",
                    "type": "number"
                }
            }
        }
        """
        html = raw_data.get('content', {}).get('html', '')
        if not html:
            return self._normalize_default(raw_data)
        
        soup = BeautifulSoup(html, 'lxml')
        custom_extractors = config.get('custom_extractors', {})
        
        items = []
        item_selector = config.get('item_selector', 'body')
        item_elements = soup.select(item_selector)
        
        for idx, item_elem in enumerate(item_elements, 1):
            item = {
                'id': f"item_{idx}",
                'title': raw_data.get('title', ''),
                'content': item_elem.get_text(separator='\n', strip=True),
                'html': str(item_elem),
                'metadata': {}
            }
            
            # åº”ç”¨è‡ªå®šä¹‰æå–å™¨
            for key, extractor_config in custom_extractors.items():
                selector = extractor_config.get('selector')
                extractor_type = extractor_config.get('type', 'text')
                
                if selector:
                    elem = item_elem.select_one(selector)
                    if elem:
                        value = elem.get_text(strip=True)
                        
                        # ç±»å‹è½¬æ¢
                        if extractor_type == 'number':
                            try:
                                value = float(re.sub(r'[^\d.]', '', value))
                            except:
                                pass
                        elif extractor_type == 'int':
                            try:
                                value = int(re.sub(r'[^\d]', '', value))
                            except:
                                pass
                        
                        item['metadata'][key] = value
            
            item['extracted_at'] = datetime.now().isoformat()
            items.append(item)
        
        if not items:
            return self._normalize_default(raw_data)
        
        return {
            **raw_data,
            'normalized': {
                'format': 'custom',
                'items': items,
                'total_count': len(items),
                'extraction_info': {
                    'method': 'custom',
                    'extractors_used': list(custom_extractors.keys())
                }
            }
        }
