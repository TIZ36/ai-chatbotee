package llm
package llm

import (
	"context"
	"fmt"
	"sync"




























































































































































































































































































































}	SystemPrompt     string	PresencePenalty  float64	FrequencyPenalty float64	TopP             float64	MaxTokens        int	Temperature      float64	BaseURL          string	APIKey           string	Model            string	Provider         string	Name             string	ID               stringtype LLMConfig struct {// LLMConfig represents LLM configuration}	return nil	// TODO: Save to database via DBC servicefunc (s *Service) SaveLLMConfig(ctx context.Context, cfg *LLMConfig) error {// SaveLLMConfig saves LLM configuration to database}	}, nil		MaxTokens:   4096,		Temperature: 0.7,		Model:       "deepseek-chat",		Provider:    "deepseek",		ID:          configID,	return &LLMConfig{	// TODO: Load from database via DBC servicefunc (s *Service) GetLLMConfig(ctx context.Context, configID string) (*LLMConfig, error) {// GetLLMConfig retrieves LLM configuration from database}	return p, nil	s.providerCache[name] = p	}		return nil, err	if err != nil {	p, err := s.cfg.Registry.Get(name)	}		return p, nil	if p, ok := s.providerCache[name]; ok {	// Double-check	defer s.mu.Unlock()	s.mu.Lock()	s.mu.RUnlock()	}		return p, nil		s.mu.RUnlock()	if p, ok := s.providerCache[name]; ok {	s.mu.RLock()func (s *Service) getProvider(name string) (llm.Provider, error) {// getProvider gets a provider by name}	Models []string	Name   stringtype ProviderInfo struct {// ProviderInfo contains information about a provider}	return provider.CountTokens(ctx, model, text)	}		return 0, err	if err != nil {	provider, err := s.getProvider(providerName)func (s *Service) CountTokens(ctx context.Context, providerName, model, text string) (int, error) {// CountTokens counts tokens in a message}	return provider.ListModels(ctx)	}		return nil, err	if err != nil {	provider, err := s.getProvider(providerName)func (s *Service) ListModels(ctx context.Context, providerName string) ([]string, error) {// ListModels returns a list of models for a provider}	return result, nil		}		})			Models: models,			Name:   name,		result = append(result, ProviderInfo{				}			continue			s.cfg.Logger.Warn("Failed to list models", "provider", name, "error", err)		if err != nil {		models, err := provider.ListModels(ctx)	for name, provider := range providers {		result := make([]ProviderInfo, 0, len(providers))	providers := s.cfg.Registry.List()func (s *Service) ListProviders(ctx context.Context) ([]ProviderInfo, error) {// ListProviders returns a list of available providers}	})		return handler(chunk)		}			}				TotalTokens:      event.Usage.TotalTokens,				CompletionTokens: event.Usage.CompletionTokens,				PromptTokens:     event.Usage.PromptTokens,			chunk.Usage = &Usage{		if event.Usage != nil {		}			Done:  event.Done,			Delta: event.Delta,			ID:    event.ID,		chunk := &StreamChunk{	return provider.ChatStream(ctx, llmReq, func(event llm.StreamEvent) error {	// Stream from provider	}		MaxTokens:   req.MaxTokens,		Temperature: req.Temperature,		Messages:    messages,		Model:       req.Model,	llmReq := &llm.ChatRequest{	// Build request	}		}			Content: m.Content,			Role:    m.Role,		messages[i] = llm.Message{	for i, m := range req.Messages {	messages := make([]llm.Message, len(req.Messages))	// Convert messages	}		return err	if err != nil {	provider, err := s.getProvider(req.Provider)func (s *Service) ChatStream(ctx context.Context, req *ChatRequest, handler func(*StreamChunk) error) error {// ChatStream performs a streaming chat completion}	return result, nil	}		})			Arguments: tc.Arguments,			Name:      tc.Name,			ID:        tc.ID,		result.ToolCalls = append(result.ToolCalls, ToolCall{	for _, tc := range resp.Message.ToolCalls {	}		},			TotalTokens:      resp.Usage.TotalTokens,			CompletionTokens: resp.Usage.CompletionTokens,			PromptTokens:     resp.Usage.PromptTokens,		Usage: Usage{		Role:    resp.Message.Role,		Content: resp.Message.Content,		Model:   resp.Model,		ID:      resp.ID,	result := &ChatResponse{	// Convert response	}		return nil, fmt.Errorf("LLM chat failed: %w", err)	if err != nil {	resp, err := provider.Chat(ctx, llmReq)	// Call provider	}		Tools:       tools,		MaxTokens:   req.MaxTokens,		Temperature: req.Temperature,		Messages:    messages,		Model:       req.Model,	llmReq := &llm.ChatRequest{	// Build request	}		}			Parameters:  t.Parameters,			Description: t.Description,			Name:        t.Name,		tools[i] = llm.Tool{	for i, t := range req.Tools {	tools := make([]llm.Tool, len(req.Tools))	// Convert tools	}		}			})				Arguments: tc.Arguments,				Name:      tc.Name,				ID:        tc.ID,			messages[i].ToolCalls = append(messages[i].ToolCalls, llm.ToolCall{		for _, tc := range m.ToolCalls {		}			ToolCallID: m.ToolCallID,			Content:    m.Content,			Role:       m.Role,		messages[i] = llm.Message{	for i, m := range req.Messages {	messages := make([]llm.Message, len(req.Messages))	// Convert messages	}		return nil, err	if err != nil {	provider, err := s.getProvider(req.Provider)func (s *Service) Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error) {// Chat performs a chat completion}	Usage     *Usage	Done      bool	ToolCall  *ToolCall	Delta     string	ID        stringtype StreamChunk struct {// StreamChunk represents a streaming response chunk}	TotalTokens      int	CompletionTokens int	PromptTokens     inttype Usage struct {// Usage represents token usage}	Usage     Usage	ToolCalls []ToolCall	Role      string	Content   string	Model     string	ID        stringtype ChatResponse struct {// ChatResponse represents a chat response}	Arguments string	Name      string	ID        stringtype ToolCall struct {// ToolCall represents a tool call from the LLM}	Parameters  map[string]interface{}	Description string	Name        stringtype ToolDefinition struct {// ToolDefinition defines a tool for function calling}	ToolCallID string	ToolCalls  []ToolCall	Content    string	Role       stringtype Message struct {// Message represents a chat message}	Stream       bool	Tools        []ToolDefinition	MaxTokens    int	Temperature  float64	Messages     []Message	Model        string	Provider     stringtype ChatRequest struct {// ChatRequest represents a chat request}	_ = svc	_ = server	// pb.RegisterLLMServiceServer(server, svc)	// Will register proto-generated service interfacefunc RegisterGRPC(server *grpc.Server, svc *Service) {// RegisterGRPC registers the service with a gRPC server}	}		providerCache: make(map[string]llm.Provider),		cfg:           cfg,	return &Service{func NewService(cfg Config) *Service {// NewService creates a new LLM service}	mu            sync.RWMutex	providerCache map[string]llm.Provider	cfg           Configtype Service struct {// Service implements the LLM gRPC service}	Logger   log.Logger	Pools    *pool.PoolManager	Registry *llm.Registrytype Config struct {// Config holds the LLM service configuration)	"google.golang.org/grpc"	"chatee-go/commonlib/pool"	"chatee-go/commonlib/log"	"chatee-go/commonlib/llm"