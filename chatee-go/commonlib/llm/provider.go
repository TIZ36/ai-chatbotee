package llm
package llm

import (
	"context"
	"encoding/json"











































































































































































































































































































































































































































































































}	}		return nil, fmt.Errorf("unknown provider type: %s", config.Type)	default:	// TODO: Add Anthropic, Gemini, etc.		}), nil			Name:    "openrouter",			BaseURL: "https://openrouter.ai/api/v1",			APIKey:  config.APIKey,		return NewOpenAIProvider(OpenAIConfig{	case "openrouter":		}), nil			Name:    "deepseek",			BaseURL: baseURL,			APIKey:  config.APIKey,		return NewOpenAIProvider(OpenAIConfig{		}			baseURL = "https://api.deepseek.com"		if baseURL == "" {		baseURL := config.BaseURL	case "deepseek":		}), nil			Name:    "openai",			BaseURL: config.BaseURL,			APIKey:  config.APIKey,		return NewOpenAIProvider(OpenAIConfig{	case "openai":	switch strings.ToLower(config.Type) {func CreateProvider(config ProviderConfig) (Provider, error) {// CreateProvider creates a provider from config.}	Options map[string]string `json:"options,omitempty"`	BaseURL string            `json:"base_url,omitempty"`	APIKey  string            `json:"api_key"`	Type    string            `json:"type"` // openai, anthropic, deepseek, etc.type ProviderConfig struct {// ProviderConfig is a generic provider configuration.// =============================================================================// Factory Functions// =============================================================================}	return names	}		names = append(names, name)	for name := range r.providers {	names := make([]string, 0, len(r.providers))		defer r.mu.RUnlock()	r.mu.RLock()func (r *Registry) List() []string {// List returns all registered provider names.}	return p, ok	p, ok := r.providers[strings.ToLower(name)]	defer r.mu.RUnlock()	r.mu.RLock()func (r *Registry) Get(name string) (Provider, bool) {// Get returns a provider by name.}	r.providers[strings.ToLower(name)] = provider	defer r.mu.Unlock()	r.mu.Lock()func (r *Registry) Register(name string, provider Provider) {// Register registers a provider.}	}		providers: make(map[string]Provider),	return &Registry{func NewRegistry() *Registry {// NewRegistry creates a new provider registry.}	mu        sync.RWMutex	providers map[string]Providertype Registry struct {// Registry manages LLM providers.// =============================================================================// Provider Registry// =============================================================================}	}		},			TotalTokens:      resp.Usage.TotalTokens,			CompletionTokens: resp.Usage.CompletionTokens,			PromptTokens:     resp.Usage.PromptTokens,		Usage: Usage{		FinishReason: string(choice.FinishReason),		Message:      msg,		Model:        resp.Model,		ID:           resp.ID,	return &ChatResponse{	}		}			}				},					Arguments: tc.Function.Arguments,					Name:      tc.Function.Name,				Function: FunctionCall{				Type: string(tc.Type),				ID:   tc.ID,			msg.ToolCalls[i] = ToolCall{		for i, tc := range choice.Message.ToolCalls {		msg.ToolCalls = make([]ToolCall, len(choice.Message.ToolCalls))	if len(choice.Message.ToolCalls) > 0 {	}		Content: choice.Message.Content,		Role:    choice.Message.Role,	msg := Message{	choice := resp.Choices[0]	}		return &ChatResponse{}	if len(resp.Choices) == 0 {func (p *OpenAIProvider) parseResponse(resp *openai.ChatCompletionResponse) *ChatResponse {// parseResponse converts OpenAI response.}	return openaiReq	}		openaiReq.Stop = req.Stop	if len(req.Stop) > 0 {	}		openaiReq.TopP = float32(*req.TopP)	if req.TopP != nil {	}		openaiReq.MaxTokens = *req.MaxTokens	if req.MaxTokens != nil {	}		openaiReq.Temperature = float32(*req.Temperature)	if req.Temperature != nil {	}		openaiReq.Tools = tools		}			}				},					Parameters:  json.RawMessage(params),					Description: t.Function.Description,					Name:        t.Function.Name,				Function: &openai.FunctionDefinition{				Type: openai.ToolTypeFunction,			tools[i] = openai.Tool{			params, _ := json.Marshal(t.Function.Parameters)		for i, t := range req.Tools {		tools := make([]openai.Tool, len(req.Tools))	if len(req.Tools) > 0 {	}		Messages: messages,		Model:    req.Model,	openaiReq := openai.ChatCompletionRequest{	}		messages[i] = msg		}			}				}					},						Arguments: tc.Function.Arguments,						Name:      tc.Function.Name,					Function: openai.FunctionCall{					Type: openai.ToolType(tc.Type),					ID:   tc.ID,				msg.ToolCalls[j] = openai.ToolCall{			for j, tc := range m.ToolCalls {			msg.ToolCalls = make([]openai.ToolCall, len(m.ToolCalls))		if len(m.ToolCalls) > 0 {		}			msg.ToolCallID = m.ToolCallID		if m.ToolCallID != "" {		}			Name:    m.Name,			Content: m.Content,			Role:    m.Role,		msg := openai.ChatCompletionMessage{	for i, m := range req.Messages {	messages := make([]openai.ChatCompletionMessage, len(req.Messages))func (p *OpenAIProvider) buildRequest(req *ChatRequest) openai.ChatCompletionRequest {// buildRequest converts ChatRequest to OpenAI format.}	return total, nil	}		total += len(m.Content) / 4	for _, m := range messages {	total := 0	// Simple estimation: ~4 chars per tokenfunc (p *OpenAIProvider) CountTokens(ctx context.Context, messages []Message) (int, error) {// CountTokens estimates token count (simplified).}	return models, nil	}		})			Name: m.ID,			ID:   m.ID,		models = append(models, ModelInfo{	for _, m := range resp.Models {	models := make([]ModelInfo, 0, len(resp.Models))	}		return nil, err	if err != nil {	resp, err := p.client.ListModels(ctx)func (p *OpenAIProvider) ListModels(ctx context.Context) ([]ModelInfo, error) {// ListModels lists available models.}	return ch, nil	}()		}			}				return				}					FinishReason: string(choice.FinishReason),					Type:         "done",				ch <- StreamEvent{				}					}						}							ToolCall: &tc,							Type:     "tool_call",						ch <- StreamEvent{					for _, tc := range currentToolCalls {				if choice.FinishReason == openai.FinishReasonToolCalls {			if choice.FinishReason != "" {			// Check finish reason			}				}					currentToolCalls[idx].Function.Arguments += tc.Function.Arguments					idx := int(*tc.Index)					// Append to existing tool call arguments				if tc.Index != nil && int(*tc.Index) < len(currentToolCalls) {				}					})						},							Name: tc.Function.Name,						Function: FunctionCall{						Type: "function",						ID:   tc.ID,					currentToolCalls = append(currentToolCalls, ToolCall{					// New tool call				if tc.Index != nil && int(*tc.Index) >= len(currentToolCalls) {			for _, tc := range delta.ToolCalls {			// Handle tool calls			}				}					Delta: delta.Content,					Type:  "content",				ch <- StreamEvent{			if delta.Content != "" {			// Handle content			delta := choice.Delta			choice := response.Choices[0]			}				continue			if len(response.Choices) == 0 {			}				return				ch <- StreamEvent{Type: "error", Error: err}			if err != nil {			}				return				ch <- StreamEvent{Type: "done"}			if err == io.EOF {			response, err := stream.Recv()		for {		var currentToolCalls []ToolCall		defer stream.Close()		defer close(ch)	go func() {	ch := make(chan StreamEvent, 100)	}		return nil, err	if err != nil {	stream, err := p.client.CreateChatCompletionStream(ctx, openaiReq)	openaiReq.Stream = true	openaiReq := p.buildRequest(req)func (p *OpenAIProvider) ChatStream(ctx context.Context, req *ChatRequest) (<-chan StreamEvent, error) {// ChatStream sends a streaming chat completion request.}	return p.parseResponse(&resp), nil	}		return nil, err	if err != nil {	resp, err := p.client.CreateChatCompletion(ctx, openaiReq)	openaiReq := p.buildRequest(req)func (p *OpenAIProvider) Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error) {// Chat sends a chat completion request.}	return p.namefunc (p *OpenAIProvider) Name() string {// Name returns the provider name.}	}		name:    name,		baseURL: config.BaseURL,		client:  openai.NewClientWithConfig(cfg),	return &OpenAIProvider{	}		name = "openai"	if name == "" {	name := config.Name	}		cfg.OrgID = config.OrgID	if config.OrgID != "" {		}		cfg.BaseURL = config.BaseURL	if config.BaseURL != "" {		cfg := openai.DefaultConfig(config.APIKey)func NewOpenAIProvider(config OpenAIConfig) *OpenAIProvider {// NewOpenAIProvider creates a new OpenAI provider.}	Name    string // Provider name (e.g., "openai", "deepseek")	OrgID   string	BaseURL string	APIKey  stringtype OpenAIConfig struct {// OpenAIConfig configures the OpenAI provider.}	name    string	baseURL string	client  *openai.Clienttype OpenAIProvider struct {// OpenAIProvider implements Provider for OpenAI API.// =============================================================================// OpenAI Provider// =============================================================================}	MaxOutput   int    `json:"max_output,omitempty"`	ContextSize int    `json:"context_size,omitempty"`	Description string `json:"description,omitempty"`	Name        string `json:"name"`	ID          string `json:"id"`type ModelInfo struct {// ModelInfo represents model information.}	Error        error      `json:"error,omitempty"`	FinishReason string     `json:"finish_reason,omitempty"`	ToolCall     *ToolCall  `json:"tool_call,omitempty"`	Delta        string     `json:"delta,omitempty"`	Type         string     `json:"type"` // content, tool_call, done, errortype StreamEvent struct {// StreamEvent represents a streaming event.}	TotalTokens      int `json:"total_tokens"`	CompletionTokens int `json:"completion_tokens"`	PromptTokens     int `json:"prompt_tokens"`type Usage struct {// Usage represents token usage.}	Usage        Usage     `json:"usage"`	FinishReason string    `json:"finish_reason"` // stop, length, tool_calls	Message      Message   `json:"message"`	Model        string    `json:"model"`	ID           string    `json:"id"`type ChatResponse struct {// ChatResponse represents a chat completion response.}	Options map[string]any `json:"options,omitempty"`	// Provider-specific options	Stream      bool      `json:"stream,omitempty"`	Stop        []string  `json:"stop,omitempty"`	TopP        *float64  `json:"top_p,omitempty"`	MaxTokens   *int      `json:"max_tokens,omitempty"`	Temperature *float64  `json:"temperature,omitempty"`	Tools       []Tool    `json:"tools,omitempty"`	Messages    []Message `json:"messages"`	Model       string    `json:"model"`type ChatRequest struct {// ChatRequest represents a chat completion request.}	Parameters  any    `json:"parameters,omitempty"` // JSON Schema	Description string `json:"description,omitempty"`	Name        string `json:"name"`type FunctionDef struct {// FunctionDef defines a function.}	Function FunctionDef  `json:"function"`	Type     string       `json:"type"` // functiontype Tool struct {// Tool represents an available tool.}	Arguments string `json:"arguments"` // JSON string	Name      string `json:"name"`type FunctionCall struct {// FunctionCall represents a function call.}	Function FunctionCall   `json:"function"`	Type     string         `json:"type"` // function	ID       string         `json:"id"`type ToolCall struct {// ToolCall represents a tool call.}	ToolCallID string     `json:"tool_call_id,omitempty"`	ToolCalls  []ToolCall `json:"tool_calls,omitempty"`	Name       string     `json:"name,omitempty"`	Content    string     `json:"content"`	Role       string     `json:"role"` // system, user, assistant, tooltype Message struct {// Message represents a chat message.// =============================================================================// Common Types// =============================================================================}	CountTokens(ctx context.Context, messages []Message) (int, error)	// CountTokens counts tokens in the messages.	ListModels(ctx context.Context) ([]ModelInfo, error)	// ListModels lists available models.	ChatStream(ctx context.Context, req *ChatRequest) (<-chan StreamEvent, error)	// ChatStream sends a streaming chat completion request.	Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error)	// Chat sends a chat completion request.	Name() string	// Name returns the provider name.type Provider interface {// Provider is the interface for LLM providers.// =============================================================================// LLM Provider Interface// =============================================================================)	"github.com/sashabaranov/go-openai"	"sync"	"strings"	"io"	"fmt"